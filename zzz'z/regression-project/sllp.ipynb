{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable | Description |\n",
    "|---|---|\n",
    "|'beds'| Number of bedrooms in home |\n",
    "|'baths'| Number of bathrooms in home including fractional bathrooms|\n",
    "|'sqft'| Calculated total finished living area of the home |\n",
    "|'fips'| Federal Information Processing Standard code -  see https://en.wikipedia.org/wiki/FIPS_county_code for more details|\n",
    "|'year'| The Year the principal residence was built |\n",
    "|'taxes'|The total property tax assessed for that assessment year|\n",
    "|'property_value'|The total tax assessed value of the parcel|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Project\n",
    "\n",
    "### codeup/innis - 2020 mar 30\n",
    "\n",
    "---\n",
    " \n",
    "## Table of Contents\n",
    "\n",
    "--- \n",
    "## I. Objective : \n",
    "\n",
    "\"We want to be able to predict the property values ('taxvaluedollarcnt') of Single Family Properties that had a transaction during 2017.\"  \n",
    "> https://ds.codeup.com/regression/project/\n",
    "\n",
    "- a.k.a: eliminate the zestimate\n",
    "- a.k.a: zestimate don't rate\n",
    "- a.k.a: \"zestimate\", more like, 'let me rest, mate' (because their models performance is snoozing on the job compared to ours ='P ) \n",
    "---\n",
    "\n",
    "## II. Dataset : Zillow  \n",
    "\n",
    "- ### Description: \n",
    "\n",
    "\tproperties_2017.csv - all the properties with their home features for 2017 (released on 10/2/2017)\n",
    "\n",
    "- ### Profile :\n",
    "\n",
    "\t\"Zillow’s Zestimate home valuation has shaken up the U.S. real estate industry since first released 11 years ago.\n",
    "\n",
    "\tA home is often the largest and most expensive purchase a person makes in his or her lifetime. Ensuring homeowners have a trusted way to monitor this asset is incredibly important. The Zestimate was created to give consumers as much information as possible about homes and the housing market, marking the first time consumers had access to this type of home value information at no cost.\n",
    "\n",
    "\t“Zestimates” are estimated home values based on 7.5 million statistical and machine learning models that analyze hundreds of data points on each property. And, by continually improving the median margin of error (from 14% at the onset to 5% today), Zillow has since become established as one of the largest, most trusted marketplaces for real estate information in the U.S. and a leading example of impactful machine learning.\"\n",
    "\n",
    "\t> https://www.kaggle.com/competitions/zillow-prize-1/data\n",
    "\n",
    "---\n",
    "\n",
    "## III. INITIAL QUESTIONS:\n",
    "\n",
    "### Data-Focused Questions\n",
    "\n",
    "- [x] Why do some properties have a much higher value than others when they are located so close to each other? \n",
    "- [] Does sqaure footage effect property value? \n",
    "- [] Does number of baths effect property value?\n",
    "- [] Does number of beds effect property value?\n",
    "- [] What is the optimal ratio of beds/baths?\n",
    "- [] Does location effect property value?\n",
    "- [] Does year constructed effect property value?\n",
    "\n",
    " \n",
    "### Overall Project-Focused Questions\n",
    "\n",
    "- What will the end product look like?\n",
    "- What format will it be in?\n",
    "- Who will it be delivered to?\n",
    "- How will it be used?\n",
    "- How will I know I'm done?\n",
    "- What is my MVP?\n",
    "- How will I know it's good enough?\n",
    " \n",
    "---\n",
    "\n",
    "## IV. FORMULATING HYPOTHESES\n",
    "Initial hypothesis was sqaure footage would effect property value. Further investigation found this to be true. Other feature's we're iddintified as well, such as: beds, baths, location, and year built. \n",
    "\n",
    "---\n",
    " \n",
    "#### 5. DELIVERABLES:\n",
    "- [] Github Repo - containing a final report (.ipynb), acquire & prepare Modules (.py), other supplemental artifacts created while working on the project (e.g. exploratory/modeling notebook(s)).\n",
    "- [] README file - provides an overview of the project and steps for project reproduction. \n",
    "- [] Draft Jupyter Notebooks - provide all steps taken to produce the project.\n",
    "- [] Python Module File - provides reproducible code for acquiring,  preparing, exploring, & testing the data.\n",
    "- [] acquire.py - used to acquire data\n",
    "- [] prepare.py - used to prepare data\n",
    "- [] Report Jupyter Notebook - provides final presentation-ready assessment and recommendations.\n",
    "- [] 5 minute presentation to stakeholders (Zillow Data Science Team. \n",
    " \n",
    " \n",
    "## II. PROJECT DATA CONTEXT\n",
    " \n",
    "#### 1. DATA DICTIONARY:\n",
    "The final DataFrame used to explore the data for this project contains the following variables (columns).  The variables, along with their data types, are defined below:\n",
    " \n",
    " \n",
    "| Variable          | Definition                                         |\n",
    "|:------------------|:--------------------------------------------------:|\n",
    "| beds          | number of bedrooms in the home                     |\n",
    "| baths         | number of bathrooms and half-bathrooms in home     |\n",
    "| fips              | federal information processing standards code      |\n",
    "| property_id       | unique identifier for each property                |\n",
    "| sqft       | total finished living area of the home             |\n",
    "| taxes    | property taxes based on assessed value in USD      |\n",
    "| property_value   | total tax assessed value of the property           |\n",
    "\n",
    "## III. PROJECT PLAN - USING THE DATA SCIENCE PIPELINE:\n",
    "I'll use the folllowing procedure: \n",
    " \n",
    "Plan➜ Acquire ➜ Prepare ➜ Explore ➜ Model & Evaluate ➜ Deliver\n",
    " \n",
    "#### 1. PLAN\n",
    "\n",
    " \n",
    "#### 2. ACQUIRE\n",
    "- []  Create acquire.py module\n",
    "- []  Store functions needed to acquire the Zillow dataset from mySQL\n",
    "- []  Ensure all imports needed to run the functions are inside the acquire.py document\n",
    "- []  Using Jupyter Notebook\n",
    "- []  Run all required imports\n",
    "- []  Import functions from aquire.py module\n",
    "- []  Summarize dataset using methods and document observations\n",
    " \n",
    "#### 3. PREPARE\n",
    "Using Python Scripting Program (VS Code)\n",
    "- [] Create prepare.py module\n",
    "- [] Store functions needed to prepare the Zillow data such as:\n",
    "   - [] Split Function: to split data into train, validate, and test\n",
    "   - [] Cleaning Function: to clean data for exploration\n",
    "   - [] Encoding Function: to create numeric columns for object column\n",
    "   - [] Feature Engineering Function: to create new features\n",
    "- [] Ensure all imports needed to run the functions are inside the prepare.py document\n",
    "Using Jupyter Notebook\n",
    "- [] Import functions from prepare.py module\n",
    "- [] Summarize dataset using methods and document observations\n",
    "- [] Clean data\n",
    "- [] Features need to be turned into numbers\n",
    "- [] Categorical features or discrete features need to be numbers that represent those categories\n",
    "- [] Continuous features may need to be standardized to compare like datatypes\n",
    "- [] Address missing values, data errors, unnecessary data, renaming\n",
    "- [] Split data into train, validate, and test samples\n",
    " \n",
    "#### 4. EXPLORE\n",
    "Using Jupyter Notebook:\n",
    "- [] Answer key questions about hypotheses and find drivers of churn\n",
    "  - Run at least two statistical tests\n",
    "  - Document findings\n",
    "- [] Create visualizations with intent to discover variable relationships\n",
    "  - Identify variables related _______________\n",
    "  - Identify any potential data integrity issues\n",
    "- [] Summarize conclusions, provide clear answers, and summarize takeaways\n",
    "  - Explain plan of action as deduced from work to this point\n",
    " \n",
    "#### 5. MODEL & EVALUATE\n",
    "Using Jupyter Notebook:\n",
    "- [] Establish baseline accuracy\n",
    "- [] Train and fit multiple (3+) models with varying algorithms and/or hyperparameters\n",
    "- [] Compare evaluation metrics across models\n",
    "- [] Remove unnecessary features\n",
    "- [] Evaluate best performing models using validate set\n",
    "- [] Choose best performing validation model for use on test set\n",
    "- [] Test final model on out-of-sample testing dataset\n",
    "- [] Summarize performance\n",
    "- [] Interpret and document findings\n",
    " \n",
    "#### 6. DELIVERY\n",
    "- [] Prepare five minute presentation using Jupyter Notebook\n",
    "- [] Include introduction of project and goals\n",
    "- [] Provide executive summary of findings, key takeaways, and recommendations\n",
    "- [] Create walk through of analysis \n",
    "  - Visualize relationships\n",
    "  - Document takeaways\n",
    "  - Explicitly define questions asked during initial analysis\n",
    "- [] Provide final takeaways, recommend course of action, and next steps\n",
    "- [] Be prepared to answer questions following presentation\n",
    "\n",
    " \n",
    " \n",
    "## IV. PROJECT MODULES:\n",
    "- [] Python Module Files - provide reproducible code for acquiring,  preparing, exploring, & testing the data.\n",
    "   - [] acquire.py - used to acquire data\n",
    "   - [] prepare.py - used to prepare data\n",
    " \n",
    "  \n",
    "## V. PROJECT REPRODUCTION:\n",
    "### Steps to Reproduce\n",
    " \n",
    "- [] You will need an env.py file that contains the hostname, username, and password of the mySQL database that contains the telco_churn database\n",
    "- [] Store that env file locally in the repository\n",
    "- [] Make .gitignore and confirm .gitignore is hiding your env.py file\n",
    "- [] Clone my repo (including the acquire.py and prepare.py)\n",
    "- [] Import python libraries:  pandas, matplotlib, seaborn, numpy, and sklearn\n",
    "- [] Follow steps as outlined in the README.md. and Churn_Work.ipynb\n",
    "- [] Run Zillow_Report.ipynb to view the final product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stat is"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
